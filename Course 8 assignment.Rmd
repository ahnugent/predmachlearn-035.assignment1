---
title: "Classification of Weightlifting Technique"
author: "A. Nugent, predmachlearn-035"
date: "Sunday, December 27, 2015"
output: html_document
---


# Purpose  

To build a predictive model for assessing quality of weightlifting technique based on a training set 
that was classified by a subject matter expert.

# Methods  

## Data  

The dataset was generated by Velloso et al. in 2013, using 6 healthy adult males who were coached to 
perform dumbell curls in 5 different ways, graded in quality from A to E. There were 19622 observations 
of 160 variables in the raw training and test sets. The raw data included 7 columns of identifying factors 
(subjects, timestamps, and time windows) plus a number of columns containing NA or "DIV/0!". 

For assessment purposes, the test set of 20 observations was provided blind.

## Preprocessing

The identifying columns named above were removed. All "skewness" and "kurtosis" columns (12 each) were 
removed, as these appeared to be junk (i.e. contained factors instead of continuous measurements, or "DIV/0!"). 
All remaining columns with NAs were removed. This left 52 variables to use as predictors, plus the output 
(classification) variable "classe".

Lacking sufficient domain knowledge, no transformations were applied to the remaining data.

The only nonstandard prerequisite package used was randomForest.

```{r, echo=FALSE}
#rm(list=ls())
setwd("E:/R_data/Course8_Ass1")
library("randomForest")

# Data cleaning functions ...

test.column <- function(v, find.what = "NA", find.how = "any")
{
    if(find.how == "any") { fun <- any }
    if(find.how == "all") { fun <- all }
    
    if(find.what == "NA")
    {
        found <- (fun(is.na(v)))
    }
    if(find.what == "NULL")
    {
        found <- (fun(is.null(v)))
    }  
    if(find.what == "ZERO")
    {
        found <- (fun(v == 0))
    }
    if(find.what == "NEGATIVE")
    {
        found <- (fun(v < 0))
    }
    if(find.what == "#DIV/0!")
    {
        found <- (fun(v == "#DIV/0!"))
    } 
    return(found)
}


dirty.columns <- function(df, find.what = "NA", find.how = "any", output) 
{
    out.l <- list()
    for (i in 1:ncol(df)) {
        test <- test.column(df[ , i], find.what = find.what, find.how = find.how)
        if (test) 
        {
            out.l[length(out.l) + 1] <- i
        }
    }
    if (output == "list") {
        out <- out.l
    }
    if (output == "vector") {
        out <- as.numeric(out.l)
    }
    return(out)
}

# Input ...

data.dir <- "data"
file.train <- "pml-training.csv"
file.test <- "pml-testing.csv"

if (!file.exists(paste0(data.dir, "/", file.train))) {
    url = "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
    download.file(url, destfile = paste0(data.dir, "/", file.train))
}
if (!file.exists(paste0(data.dir, "/", file.test))) {
    url = "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
    download.file(url, destfile = paste0(data.dir, "/", file.test))
}

data.train.all <- read.csv(paste0(data.dir, "/", file.train))
data.test.all <- read.csv(paste0(data.dir, "/", file.test))

# remove non-predictive and dirty columns ...

not.used1 <- c("X", "user_name", "raw_timestamp_part_1", "raw_timestamp_part_2",
              "cvtd_timestamp", "new_window", "num_window", "amplitude_yaw_dumbbell", 
              "min_yaw_forearm", "amplitude_yaw_forearm")
not.used2 <- grep("skewness", names(data.train.all), value = TRUE)
not.used3 <- grep("kurtosis", names(data.train.all), value = TRUE)
not.used <- c(not.used1, not.used2, not.used3)
data.train <- data.train.all[ , -which(names(data.train.all) %in% not.used)]

not.used4 <- grep("_yaw", names(data.train), value = TRUE)
data.train <- data.train[ , -which(names(data.train) %in% not.used4)]

na.cols <- dirty.columns(data.train, output = "vector")
data.train <- data.train[ , -na.cols]

not.used2 <- grep("skewness", names(data.test.all), value = TRUE)
not.used3 <- grep("kurtosis", names(data.test.all), value = TRUE)
not.used <- c(not.used1, not.used2, not.used3)
data.test <- data.test.all[ , -which(names(data.test.all) %in% not.used)]

not.used4 <- grep("_yaw", names(data.test), value = TRUE)
data.test <- data.test[ , -which(names(data.test) %in% not.used4)]

na.cols <- dirty.columns(data.test, output = "vector")
data.test <- data.test[ , -na.cols]
```

## Model Selection

The random forest model was chosen for its demonstrated success in building classification 
models. To determine the optimal number of variables to use as predictors, cross-validation was 
invoked using the rfcv function from the randomForest package:

```{r chunk1, cache=TRUE}
cv <- rfcv(data.train[ , -which(names(data.train) == "classe")], data.train$classe)
```

It was apparent that 26 variables would be sufficient for prediction, as the improvement 
in accuracy from 26 to 52 predictors was only 0.001 (0.1 percentage points). The cross-
validation results are plotted below.

```{r, echo=FALSE}
with(cv, plot(n.var, error.cv, log="x", type="o", lwd=2, col="blue",
              xlab="Number of Variables", ylab="Error Rate", main="Cross-Validation of Random Forest Modeling"))
```

These same results are tabulated below. 

```{r, echo=FALSE}
#sprintf("%.4f", cv$error.cv)
cv$error.cv
```
  
Thus, the expected out-of-sample error rate would be 0.52% with 26 variables in the random forest model. 
This also yielded an acceptable computation duration (2.5 minutes using 64-bit Windows on a 2.83-GHz Intel 
Quad CPU with 8 GB RAM). Based on this result, the choice of model and parameters was vindicated.

```{r chunk2, cache=TRUE}
modFit26 <- randomForest(data.train[ , -which(names(data.train) == "classe")], 
                        data.train$classe, mtry = 26, importance = TRUE)
```

# Results

## Model Evaluation

The following plot shows the rapid decline in error rate of each predicted class as the number of trees 
goes from zero to a little over 30. (OOB is the out-of-bag error.)
```{r}
plot(modFit26)
modFit26.legend <- if (is.null(modFit26$test$err.rate)) {colnames(modFit26$err.rate)} else {colnames(modFit26$test$err.rate)}
legend("topright", cex =0.7, legend=modFit26.legend, lty=c(1,2,3,4,5), col=c(1,2,3,4,5), horiz=FALSE)
```

The relative importance of each predictor in the model is illustrated in the following plot.
```{r}
varImpPlot(modFit26, col="blue")
```
  
It is clear that the first 6 predictors are the most valuable; their order of importance 
varies only slightly between the two measures of accuracy.  

The accuracy of the model as applied to the training set was perfect:
```{r}
predictions.train <- predict(modFit26, newdata = data.train[ , -which(names(data.train) == "classe")])
data.train$correct <- predictions.train == data.train$classe
table(predictions.train, data.train$classe)
```

## Predictions from Test Data

The resulting predictions for the test data were:
```{r}
predictions <- predict(modFit26, newdata = data.test[ , -which(names(data.test) == "problem_id")])
predictions
```

Based on the out-of-sample error estimate and the perfect fit to the training data, and assuming that no 
bias was introduced when the training and test data were originally partitioned, the 
accuracy of the model should also be 100%.

